 @article{Chen_Jin_Zhu_Luo_Wang_2020, title={Text Recognition in the Wild: A Survey}, url={http://arxiv.org/abs/2005.03492}, abstractNote={The history of text can be traced back over thousands of years. Rich and precise semantic information carried by text is important in a wide range of vision-based application scenarios. Therefore, text recognition in natural scenes has been an active research field in computer vision and pattern recognition. In recent years, with the rise and development of deep learning, numerous methods have shown promising in terms of innovation, practicality, and efficiency. This paper aims to (1) summarize the fundamental problems and the state-of-the-art associated with scene text recognition; (2) introduce new insights and ideas; (3) provide a comprehensive review of publicly available resources; (4) point out directions for future work. In summary, this literature review attempts to present the entire picture of the field of scene text recognition. It provides a comprehensive reference for people entering this field, and could be helpful to inspire future research. Related resources are available at our Github repository: https://github.com/HCIILAB/Scene-Text-Recognition. CCS Concepts: • Computer systems organization → Embedded systems; Redundancy; Robotics; • Networks → Network reliability.}, note={arXiv: 2005.03492}, journal={arXiv:2005.03492 [cs]}, author={Chen, Xiaoxue and Jin, Lianwen and Zhu, Yuanzhi and Luo, Canjie and Wang, Tianwei}, year={2020}, month={Dec} }

 @article{Yin_Yin_Huang_Hao_2014, title={Robust Text Detection in Natural Scene Images}, volume={36}, ISSN={0162-8828, 2160-9292}, DOI={10.1109/TPAMI.2013.182}, abstractNote={Text detection in natural scene images is an important prerequisite for many content-based image analysis tasks. In this paper, we propose an accurate and robust method for detecting texts in natural scene images. A fast and effective pruning algorithm is designed to extract Maximally Stable Extremal Regions (MSERs) as character candidates using the strategy of minimizing regularized variations. Character candidates are grouped into text candidates by the single-link clustering algorithm, where distance weights and threshold of the clustering algorithm are learned automatically by a novel self-training distance metric learning algorithm. The posterior probabilities of text candidates corresponding to non-text are estimated with an character classiﬁer; text candidates with high probabilities are then eliminated and ﬁnally texts are identiﬁed with a text classiﬁer. The proposed system is evaluated on the ICDAR 2011 Robust Reading Competition dataset; the f measure is over 76% and is signiﬁcantly better than the state-of-the-art performance of 71%. Experimental results on a publicly available multilingual dataset also show that our proposed method can outperform the other competitive method with the f measure increase of over 9 percent. Finally, we have setup an online demo of our proposed scene text detection system at “http://kems.ustb.edu.cn/learning/yin/dtext”.}, note={arXiv: 1301.2628}, number={5}, journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, author={Yin, Xu-Cheng and Yin, Xuwang and Huang, Kaizhu and Hao, Hong-Wei}, year={2014}, month={May}, pages={970–983} }

 @inproceedings{Kang_Li_Doermann_2014, place={Columbus, OH, USA}, title={Orientation Robust Text Line Detection in Natural Images}, ISBN={978-1-4799-5118-5}, url={https://ieeexplore.ieee.org/document/6909910}, DOI={10.1109/CVPR.2014.514}, abstractNote={In this paper, higher-order correlation clustering (HOCC) is used for text line detection in natural images. We treat text line detection as a graph partitioning problem, where each vertex is represented by a Maximally Stable Extremal Region (MSER). First, weak hypothesises are proposed by coarsely grouping MSERs based on their spatial alignment and appearance consistency. Then, higherorder correlation clustering (HOCC) is used to partition the MSERs into text line candidates, using the hypotheses as soft constraints to enforce long range interactions. We further propose a regularization method to solve the Semidefinite Programming problem in the inference. Finally we use a simple texton-based texture classiﬁer to ﬁlter out the non-text areas. This framework allows us to naturally handle multiple orientations, languages and fonts. Experiments show that our approach achieves competitive performance compared to the state of the art.}, booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, publisher={IEEE}, author={Kang, Le and Li, Yi and Doermann, David}, year={2014}, month={Jun}, pages={4034–4041} }

 @article{Kim_Nowozin_Kohli_Yoo, title={Higher-Order Correlation Clustering for Image Segmentation}, abstractNote={For many of the state-of-the-art computer vision algorithms, image segmentation is an important preprocessing step. As such, several image segmentation algorithms have been proposed, however, with certain reservation due to high computational load and many hand-tuning parameters. Correlation clustering, a graphpartitioning algorithm often used in natural language processing and document clustering, has the potential to perform better than previously proposed image segmentation algorithms. We improve the basic correlation clustering formulation by taking into account higher-order cluster relationships. This improves clustering in the presence of local boundary ambiguities. We ﬁrst apply the pairwise correlation clustering to image segmentation over a pairwise superpixel graph and then develop higher-order correlation clustering over a hypergraph that considers higher-order relations among superpixels. Fast inference is possible by linear programming relaxation, and also effective parameter learning framework by structured support vector machine is possible. Experimental results on various datasets show that the proposed higher-order correlation clustering outperforms other state-of-the-art image segmentation algorithms.}, author={Kim, Sungwoong and Nowozin, Sebastian and Kohli, Pushmeet and Yoo, Chang D}, pages={9} }

 @inbook{Neumann_Matas_2011, place={Berlin, Heidelberg}, series={Lecture Notes in Computer Science}, title={A Method for Text Localization and Recognition in Real-World Images}, volume={6494}, ISBN={978-3-642-19317-0}, DOI={10.1007/978-3-642-19318-7_60}, abstractNote={A general method for text localization and recognition in real-world images is presented. The proposed method is novel, as it (i) departs from a strict feed-forward pipeline and replaces it by a hypothesesveriﬁcation framework simultaneously processing multiple text line hypotheses, (ii) uses synthetic fonts to train the algorithm eliminating the need for time-consuming acquisition and labeling of real-world training data and (iii) exploits Maximally Stable Extremal Regions (MSERs) which provides robustness to geometric and illumination conditions.}, booktitle={Computer Vision – ACCV 2010}, publisher={Springer Berlin Heidelberg}, author={Neumann, Lukas and Matas, Jiri}, editor={Kimmel, Ron and Klette, Reinhard and Sugimoto, Akihiro}, year={2011}, pages={770–783}, collection={Lecture Notes in Computer Science} }

 @article{Matas_Chum_Urban_Pajdla, title={Robust Wide Baseline Stereo from Maximally Stable Extremal Regions}, abstractNote={The wide-baseline stereo problem, i.e. the problem of establishing correspondences between a pair of images taken from different viewpoints is studied. A new set of image elements that are put into correspondence, the so called extremal regions, is introduced. Extremal regions possess highly desirable properties: the set is closed under 1. continuous (and thus projective) transformation of image coordinates and 2. monotonic transformation of image intensities. An efﬁcient (near linear complexity) and practically fast detection algorithm (near frame rate) is presented for an afﬁnely-invariant stable subset of extremal regions, the maximally stable extremal regions (MSER).}, author={Matas, J and Chum, O and Urban, M and Pajdla, T}, pages={10} }

 @article{Li_Jia_Shen_Hengel_2013, title={Characterness: An Indicator of Text in the Wild}, url={http://arxiv.org/abs/1309.6691}, abstractNote={Text in an image provides vital information for interpreting its contents, and text in a scene can aide with a variety of tasks from navigation, to obstacle avoidance, and odometry. Despite its value, however, identifying general text in images remains a challenging research problem. Motivated by the need to consider the widely varying forms of natural text, we propose a bottom-up approach to the problem which reﬂects the ‘characterness’ of an image region. In this sense our approach mirrors the move from saliency detection methods to measures of ‘objectness’. In order to measure the characterness we develop three novel cues that are tailored for character detection, and a Bayesian method for their integration. Because text is made up of sets of characters, we then design a Markov random ﬁeld (MRF) model so as to exploit the inherent dependencies between characters.}, note={arXiv: 1309.6691}, journal={arXiv:1309.6691 [cs]}, author={Li, Yao and Jia, Wenjing and Shen, Chunhua and Hengel, Anton van den}, year={2013}, month={Sep} }

 @article{Zhou_Yao_Wen_Wang_Zhou_He_Liang_2017, title={EAST: An Efficient and Accurate Scene Text Detector}, url={http://arxiv.org/abs/1704.03155}, abstractNote={Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm signiﬁcantly outperforms state-of-the-art methods in terms of both accuracy and efﬁciency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution.}, note={arXiv: 1704.03155}, journal={arXiv:1704.03155 [cs]}, author={Zhou, Xinyu and Yao, Cong and Wen, He and Wang, Yuzhi and Zhou, Shuchang and He, Weiran and Liang, Jiajun}, year={2017}, month={Jul} }

 @article{Kim_Hong_Roh_Cheon_Park_2016, title={PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection}, url={http://arxiv.org/abs/1608.08021}, abstractNote={This paper presents how we can achieve the state-of-the-art accuracy in multicategory object detection task while minimizing the computational cost by adapting and combining recent technical innovations. Following the common pipeline of “CNN feature extraction + region proposal + RoI classiﬁcation”, we mainly redesign the feature extraction part, since region proposal part is not computationally expensive and classiﬁcation part can be efﬁciently compressed with common techniques like truncated SVD. Our design principle is “less channels with more layers” and adoption of some building blocks including concatenated ReLU, Inception, and HyperNet. The designed network is deep and thin and trained with the help of batch normalization, residual connections, and learning rate scheduling based on plateau detection. We obtained solid results on well-known object detection benchmarks: 83.8% mAP (mean average precision) on VOC2007 and 82.5% mAP on VOC2012 (2nd place), while taking only 750ms/image on Intel i7-6700K CPU with a single core and 46ms/image on NVIDIA Titan X GPU. Theoretically, our network requires only 12.3% of the computational cost compared to ResNet-101, the winner on VOC2012.}, note={arXiv: 1608.08021}, journal={arXiv:1608.08021 [cs]}, author={Kim, Kye-Hyeon and Hong, Sanghoon and Roh, Byungseok and Cheon, Yeongjae and Park, Minje}, year={2016}, month={Sep} }


 @article{Long_Shelhamer_Darrell_2015, title={Fully Convolutional Networks for Semantic Segmentation}, url={http://arxiv.org/abs/1411.4038}, abstractNote={Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixelsto-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efﬁcient inference and learning. We deﬁne and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classiﬁcation networks (AlexNet [19], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by ﬁne-tuning [4] to the segmentation task. We then deﬁne a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, ﬁne layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one ﬁfth of a second for a typical image.}, note={arXiv: 1411.4038}, journal={arXiv:1411.4038 [cs]}, author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor}, year={2015}, month={Mar} }
